{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of Gradient Descent with squared error in Python\n",
    "This notebook is a study of gradient descent technique in Python using the saqured error function.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Formula reference\n",
    "\n",
    "<br> \n",
    "\n",
    "In a simple one layer neural network(inpus and activation/output), the gradient of the squared error in respect to $w_i$ is:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_i} = -(y-\\hat{y}){f}'(h)x_i$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where : \n",
    "\n",
    "<br>\n",
    "\n",
    "$${f}'(h) = {\\sigma}'(h) = \\sigma(h)(1-\\sigma(h))$$\n",
    "\n",
    "<br>\n",
    "\n",
    "leading to :\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_i} = -(y-\\hat{y})\\sigma(h)(1-\\sigma(h))x_i$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The weight update step is given by:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\Delta w_i = \\alpha(y-{y}'){f}'(h)x_i$$\n",
    "\n",
    "where we can define for convenience, a variable \"error term\" $\\delta$, given by :\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\delta  = (y-\\hat{y}){f}'(h)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Giving us the following weight update formula:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$w_i=w_i+\\alpha\\delta x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Python implementation\n",
    "Only one input sample, using sigmoid activation function $f(h)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.6899744811276125\n",
      "Amount of Error:\n",
      "-0.1899744811276125\n",
      "Change in Weights:\n",
      "[array([-0.02031869, -0.04063738, -0.06095608, -0.08127477])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid activation function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Simulation of gradient descent\n",
    "\n",
    "learning_rate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "# Prediction or feedforward\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# NN output\n",
    "output = sigmoid(h)\n",
    "\n",
    "# Error calculation\n",
    "error = (y - output)\n",
    "\n",
    "# Error term\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "# Weights update\n",
    "del_w = [learning_rate * error_term * x]\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent applied to a Neural Network for multiple weights update - UCLA admissions dataset\n",
    "The last example was a simulation of a really small neural network with only one input sample on the input layer and running for only one epoch. \n",
    "\n",
    "<br>\n",
    "\n",
    "Now, the concept from the previous example is taken and scaled up for an example of a simple neural network with only one input layer and one activation/output layer, but this time using multiple input samples on the training and testing datasets, running over hundreads epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset load\n",
    "data = pd.read_csv('../dataset/ucla-admissions')\n",
    "# Transforms the rank column into one hot encoded columns\n",
    "rank_one_hot = pd.get_dummies(data['rank'], prefix='rank')\n",
    "# Add one hot encoded columns to main dataframe and \n",
    "# drop old categorical rank column\n",
    "data = pd.concat([data, rank_one_hot], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "# Split features and targets\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "train_data, test_data = data.iloc[sample], data.drop(sample)\n",
    "x_train = train_data.drop('admit', axis=1)\n",
    "y_train = train_data['admit']\n",
    "x_test = test_data.drop('admit', axis=1)\n",
    "y_test = test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def training(x_train, y_train, epochs=1000, learning_rate=0.5):\n",
    "    n_records, n_features = x_train.shape\n",
    "    last_loss = None\n",
    "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        del_w = np.zeros(weights.shape)\n",
    "        for x, y in zip(x_train.values, y_train):\n",
    "            # Network output or prediction\n",
    "            output = sigmoid(np.dot(x, weights))\n",
    "            # Error from previous prediction\n",
    "            error = y - output\n",
    "            # Calculate the error term\n",
    "            error_term = error * output * (1 - output)\n",
    "            # The gradient step for the current sample being analyzed\n",
    "            del_w += error_term * x\n",
    "        \n",
    "        # After analyzign all sample, update the weights based on the\n",
    "        # total error or the total gradient steps for each sample\n",
    "        weights += learning_rate * (del_w/n_records)\n",
    "        \n",
    "            # Printing out the mean square error on the training set\n",
    "        if e % (epochs / 10) == 0:\n",
    "            out = sigmoid(np.dot(x_train, weights))\n",
    "            loss = np.mean((out - y_train) ** 2)\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.27399399505027644\n",
      "Train loss:  0.21258469351613782\n",
      "Train loss:  0.2015285048367407\n",
      "Train loss:  0.19878389269310112\n",
      "Train loss:  0.19782937236263026\n",
      "Train loss:  0.19742326198825286\n",
      "Train loss:  0.19722617724052421\n",
      "Train loss:  0.19712124179879126\n",
      "Train loss:  0.19706147612351815\n",
      "Train loss:  0.19702571727085305\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "optimized_weights = training(x_train, y_train)\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(x_test, optimized_weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
