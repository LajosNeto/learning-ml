{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification metrics overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really good guide when choosing the right metric for a problem by the awesome Koo Ping Shung [7] :\n",
    "\n",
    "> __First Question:__ Does both True Positive and True Negatives matters to the business or just True Positives? If both is important, Accuracy is what you go for.\n",
    ">\n",
    "> __Second Question:__ After establishing that True Positive is what you are concerned with more, ask yourself, which one has a higher costs to business, False Positives or False Negatives?\n",
    ">\n",
    "> If having large number of False Negatives has a higher cost to business, choose Recall.\n",
    ">\n",
    "> If having large number of False Positives has a higher cost to business, choose Precision.\n",
    ">\n",
    "> If you cannot decide or thinks that its best to reduce both, False Negatives and False Positives then choose F1.\n",
    "Hope the above two questions will simplify which metrics to choose.\n",
    "\n",
    "<br>\n",
    "\n",
    "Refs.:\n",
    "\n",
    "[1] https://stackoverflow.com/a/54458777/4885494\n",
    "\n",
    "[2] https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226 (superb caveats definition)\n",
    "\n",
    "[3] https://www.kdnuggets.com/2018/06/right-metric-evaluating-machine-learning-models-2.html \n",
    "\n",
    "[4] https://developers.google.com/machine-learning/crash-course/classification/thresholding (cases and some really good insights)\n",
    "\n",
    "[5] https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "\n",
    "[6] https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9\n",
    "\n",
    "[7] https://koopingshung.com/blog/machine-learning-model-selection-accuracy-precision-recall-f1/\n",
    "\n",
    "[8] https://medium.com/@shmueli/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2\n",
    "\n",
    "[9] https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1\n",
    "\n",
    "[10] https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "\n",
    "[11] https://towardsdatascience.com/the-ultimate-guide-to-binary-classification-metrics-c25c3627dd0a#a1d0\n",
    "\n",
    "[12] https://towardsdatascience.com/evaluating-categorical-models-ii-sensitivity-and-specificity-e181e573cff8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import \n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this guide, we will be using two different classification models :\n",
    "- Binary classification (two classes)\n",
    "- Multiclass classification single-label (multiple classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary model\n",
    "breast_dataset = load_breast_cancer()\n",
    "X_breast = breast_dataset['data']\n",
    "y_breast = breast_dataset['target']\n",
    "x_b_train, x_b_test, y_b_train, y_b_test = train_test_split(X_breast, y_breast, test_size=0.3, random_state=42)\n",
    "\n",
    "binary_model = Sequential()\n",
    "binary_model.add(Dense(16, input_dim=30, activation='relu'))\n",
    "binary_model.add(Dense(8, activation='relu'))\n",
    "binary_model.add(Dense(1, activation='sigmoid'))\n",
    "binary_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "binary_model.fit(x_b_train, y_b_train, epochs=50, validation_data=(x_b_test, y_b_test), verbose=0)\n",
    "binary_preds = binary_model.predict(x_b_test)\n",
    "binary_preds = [1 if pred>0.5 else 0 for pred in binary_preds]\n",
    "\n",
    "# Multi class single-label model\n",
    "iris_dataset = load_iris()\n",
    "X_iris = iris_dataset['data']\n",
    "y_iris = to_categorical(iris_dataset['target'])\n",
    "x_i_train, x_i_test, y_i_train, y_i_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
    "\n",
    "multi_class_model = Sequential()\n",
    "multi_class_model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "multi_class_model.add(Dense(10, activation='relu'))\n",
    "multi_class_model.add(Dense(3, activation='softmax'))\n",
    "multi_class_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "multi_class_model.fit(x_i_train, y_i_train, batch_size=5, epochs=200, validation_data=(x_i_test, y_i_test), verbose=0)\n",
    "multiclass_preds = to_categorical(multi_class_model.predict_classes(x_i_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "<br>\n",
    "\n",
    "The confusion matrix is a representation of the relation between actual classes and predicted classes.\n",
    "\n",
    "Its rows represent the actual classes while the columns represet the predicted classes.\n",
    "\n",
    "<br>\n",
    "\n",
    "Basically, the confusion matrix is composed by (for binary classification):\n",
    "- True positives (TP) - when a positive class is correctly classified as a positive class.\n",
    "- True negatives (TN) - when a negative class is correclty classified as a negative class.\n",
    "- False positives (FP) - when a negative class is wrongly classified as a positive class.\n",
    "- False negatives (FN) - when a positive class is wrongly classified as a negative class.\n",
    "\n",
    "<br>\n",
    "\n",
    "For the case of multiclass classification problems, the same structure is applied, with the difference that now the shape of the confusion matrix will be NxN, given N possible classes on the network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">True Label</th>\n",
       "      <th>Negative</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>6</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Predicted         \n",
       "                     Negative Positive\n",
       "True Label Negative        59        4\n",
       "           Positive         6      102"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary classific\n",
    "cm = confusion_matrix(y_b_test, binary_preds)\n",
    "data = {'Negative': cm[:,0], 'Positive': cm[:,1]}\n",
    "df = pd.concat({\"Predicted\": pd.DataFrame(data, index=[['True Label', 'True Label'],['Negative', 'Positive']])}, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1   2\n",
       "0  10  0   0\n",
       "1   0  8   1\n",
       "2   0  0  11"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiclass classification confusion matrix\n",
    "cm2 = confusion_matrix(y_i_test.argmax(axis=1), multiclass_preds.argmax(axis=1))\n",
    "pd.DataFrame(data=cm2, columns=range(3), index=range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations for the confusion matrix from the multiclass classifier :\n",
    "- the diagonal of the confusion matrix are the true positive (TP) cases, [10, 8, 11]\n",
    "- the value from the position [1,2] (row 1 / column 2) tells us how many samples from class 1 were identified as class 2\n",
    "\n",
    "<br>\n",
    "\n",
    "Usually, the confusion matrix is represented by : columns (true label) and rows (predicted labels).\n",
    "\n",
    "In the case of the confusion matrix generated by sklearn, this matrix is kind of \"transposed\", with the columns representing the predicted classes and the rows representing the true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*accuracy = (tp + tn) / (tp + fp + tn + fn)*__\n",
    "\n",
    "Accuracy is the metrics that measures how many observations were correctly classified.\n",
    "\n",
    "This is basically the ratio between the true positives and all the other predictions (tp, fp, tn and fn).\n",
    "\n",
    "<br>\n",
    "\n",
    "__when to use__:\n",
    "- it is good to use when there is no class imbalance and no skeweness\n",
    "- can be used for both binary and multiclass classification problems\n",
    "\n",
    "__caveats__:\n",
    "- as the target distribution is very sparse, it can lead to incorrect conclusions : e.g., if we are trying to predict if an extremelly rare event will happen or not, let's say it happens 1/100 times. If our classifier only outputs 100 \"false\" labels, our accuravy will by 99% even if our real interest, the \"true\" label was misclassified.\n",
    "\n",
    "__cases__:\n",
    "- In the game of roulette, a ball is dropped on a spinning wheel and eventually lands in one of 38 slots. Using visual features (the spin of the ball, the position of the wheel when the ball was dropped, the height of the ball over the wheel), an ML model can predict the slot that the ball will land in with an accuracy of 4%.\n",
    "    - This ML model is making predictions far better than chance; a random guess would be correct 1/38 of the time—yielding an accuracy of 2.6%. Although the model's accuracy is \"only\" 4%, the benefits of success far outweigh the disadvantages of failure.\n",
    "- A deadly, but curable, medical condition afflicts .01% of the population. An ML model uses symptoms as features and predicts this affliction with an accuracy of 99.99%.\n",
    "    - Accuracy is a poor metric here. After all, even a \"dumb\" model that always predicts \"not sick\" would still be 99.99% accurate. Mistakenly predicting \"not sick\" for a person who actually is sick could be deadly.\n",
    "- An expensive robotic chicken crosses a very busy road a thousand times per day. An ML model evaluates traffic patterns and predicts when this chicken can safely cross the street with an accuracy of 99.99%.\n",
    "    - A 99.99% accuracy value on a very busy road strongly suggests that the ML model is far better than chance. In some settings, however, the cost of making even a small number of mistakes is still too high. 99.99% accuracy means that the expensive chicken will need to be replaced, on average, every 10 days. (The chicken might also cause extensive damage to cars that it hits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary model accuracy : 0.9415204678362573\n",
      "Multiclass single-label model accuracy : 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(f\"Binary model accuracy : {accuracy_score(y_b_test, binary_preds)}\")\n",
    "print(f\"Multiclass single-label model accuracy : {accuracy_score(y_i_test, multiclass_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision (Positive predictive value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*precision = tp / (tp+fp)*__\n",
    "\n",
    "The precision metric can be thought as the accuracy of the positive predictions.\n",
    "\n",
    "It is the ratio of the true positive predictions against all positive predictions, both correct and wrong ones.\n",
    "\n",
    "The precision metrics answers the following question : \"From all the positive predictions, how many of them were trully positive?\"\n",
    "\n",
    "<br>\n",
    "\n",
    "__when to use__:\n",
    "- when we want to be sure of our predictions (true predictions in the case of binary classifier)\n",
    "- Precision is a good measure to determine, when the costs of False Positive is high. For instance, email spam detection. In email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model.\n",
    "\n",
    "__caveats__:\n",
    "- Following the binary classifier example, as we are focusing on making sure that we get only the emails that trully are spams and not deleting ham emails wrongly, we end up letting some spams pass through our filter as we do not focus on the false nagtives nor true negative cases.\n",
    "\n",
    "__cases__:\n",
    "- Consider a classification model that separates email into two categories: \"spam\" or \"not spam.\" If you raise the classification threshold, what will happen to precision?\n",
    "    - Probably increase, in general, raising the classification threshold reduces false positives, thus raising precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary model precision : 0.9622641509433962\n"
     ]
    }
   ],
   "source": [
    "print(f\"Binary model precision : {precision_score(y_b_test, binary_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiclass/multilabel cases, we also need to set an extra parameter that specifies which __average__ approach will be taken.\n",
    "\n",
    "- __*micro*__ : calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "- __*macro*__ : calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "- __*weighted*__ : Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "- __*samples*__ : Calculate metrics for each instance, and find their average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass single-label precision methods\n",
      "\n",
      "micro average : 0.9666666666666667\n",
      "macro average : 0.9722222222222222\n",
      "weighted average : 0.9694444444444444\n",
      "samples average : 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "averages = ['micro', 'macro', 'weighted', 'samples']\n",
    "print(f\"Multiclass single-label precision methods\\n\")\n",
    "for avg in averages:\n",
    "    print(f\"{avg} average : {precision_score(y_i_test, multiclass_preds, average=avg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall / Sensitivity (True positive rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*recall = tp / (tp+fn)*__\n",
    "\n",
    "The recall metrics answers the following questions : \n",
    "\n",
    "\"What proportion of actual positives was identified correctly?\"\n",
    "\n",
    "\"How many positive cases wrongly passed by as negatives?\"\n",
    "\n",
    "<br>\n",
    "\n",
    "__when to use__:\n",
    "- recall metric should be used when there is a high cost associated with false negatives (FN), e.g. breast cancer detection.\n",
    "- when it is desired to capture as many postive cases as possible, even when we are not completely sure of it\n",
    "\n",
    "__caveats__:\n",
    "- If we predict true for all cases, the recall score will be 1.0, leading to misleaded conclusions about our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary model recall : 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "print(f\"Binary model recall : {recall_score(y_b_test, binary_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass single-label recall methods\n",
      "\n",
      "micro average : 0.9666666666666667\n",
      "macro average : 0.9629629629629629\n",
      "weighted average : 0.9666666666666667\n",
      "samples average : 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "averages = ['micro', 'macro', 'weighted', 'samples']\n",
    "print(f\"Multiclass single-label recall methods\\n\")\n",
    "for avg in averages:\n",
    "    print(f\"{avg} average : {recall_score(y_i_test, multiclass_preds, average=avg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity (True negative rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*specificity = tn / (tn+fp)*__\n",
    "\n",
    "Measures the proportion of actual negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition).\n",
    "\n",
    "The specificity answers the following questions :\n",
    "\n",
    "\"What proportion of actual negatives was identified correctly?\"\n",
    "\n",
    "\"How many negative cases wrongly passed by as positives?\"\n",
    "\n",
    "We could say that while the recall tells us how good we are on getting all the positive cases, the specificity tells us the same story about the negative cases.\n",
    "\n",
    "<br>\n",
    "\n",
    "__when to use__:\n",
    "- the specificity should be used as an auxiliary metric\n",
    "- when we want to be sure when we say something is safe, e.g. when our model tells that someone is negative to some desease, we want to be sure of that, otherwise we would be sending someone sick home\n",
    "\n",
    "__caveats__:\n",
    "- not very useful when used alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary model specificity : 0.9365079365079365\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_b_test, binary_preds).ravel()\n",
    "print(f\"Binary model specificity : {tn / (tn+fp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no built in method for calculating the specificity on sklearn.\n",
    "\n",
    "To calculate it to multiclass case, we just need to follow the following steps:\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://res.cloudinary.com/lajosneto/image/upload/v1581860466/learning-ml/specificity-formula.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity x Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Sensitivity_and_specificity.svg/700px-Sensitivity_and_specificity.svg.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*F1 = 2 * (precision * recall) / (precision + recall)*__\n",
    "\n",
    "The recall metrics answers the following questions : \n",
    "\n",
    "\"What proportion of actual positives was identified correctly?\"\n",
    "\n",
    "\"How many positive cases wrongly passed by as negatives?\"\n",
    "\n",
    "The following analogy helps to illustrate the F1 score metric [2] :\n",
    "> If you are a police inspector and you want to catch criminals, you want to be sure that the person you catch is a criminal (Precision) and \n",
    ">\n",
    "> you also want to capture as many criminals (Recall) as possible. The F1 score manages this tradeoff.\n",
    "\n",
    "<br>\n",
    "\n",
    "__when to use__:\n",
    "- when a balance between precision and recall is required\n",
    "- when the balance of precision x recall is desired AND there is an uneven class distribution, with far more actual negatives cases\n",
    "\n",
    "__caveats__:\n",
    "- It will give the same importance/weight for both precision and recall even though sometimes we need to set a custom weight for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary model F1 score : 0.9532710280373832\n"
     ]
    }
   ],
   "source": [
    "print(f\"Binary model F1 score : {f1_score(y_b_test, binary_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass single-label F1 score methods\n",
      "\n",
      "micro average : 0.9666666666666667\n",
      "macro average : 0.9658994032395567\n",
      "weighted average : 0.9664109121909632\n",
      "samples average : 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "averages = ['micro', 'macro', 'weighted', 'samples']\n",
    "print(f\"Multiclass single-label F1 score methods\\n\")\n",
    "for avg in averages:\n",
    "    print(f\"{avg} average : {f1_score(y_i_test, multiclass_preds, average=avg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-β Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a variation from the original F1 score, but here we can control the the tradeoff between precision and recall.\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://res.cloudinary.com/lajosneto/image/upload/v1581808821/learning-ml/fbeta-formula.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary model F-β score : 0.962084422861412\n"
     ]
    }
   ],
   "source": [
    "print(f\"Binary model F-β score : {fbeta_score(y_b_test, binary_preds, beta=0.1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass single-label F-β score methods\n",
      "\n",
      "micro average : 0.9666666666666667\n",
      "macro average : 0.9720625076341475\n",
      "weighted average : 0.9693511646604382\n",
      "samples average : 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "averages = ['micro', 'macro', 'weighted', 'samples']\n",
    "print(f\"Multiclass single-label F-β score methods\\n\")\n",
    "for avg in averages:\n",
    "    print(f\"{avg} average : {fbeta_score(y_i_test, multiclass_preds, beta=0.1, average=avg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC - ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is composed by two parts :\n",
    "- ROC, the probability curve\n",
    "- AUC, the representation of the degree/measure of separability\n",
    "\n",
    "The question it tries to answer is : \"How much is the model capable of distinguishingh between classes?\"\n",
    "\n",
    "The higher the AUC, the better it is at predicting true samples as true samples and negative samples as negative samples.\n",
    "\n",
    "<img style=\"float: left;\" src=\"\n",
    "https://res.cloudinary.com/lajosneto/image/upload/v1581826569/learning-ml/roc-auc-curve.png\" width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img style=\"float: left;\" src=\"\n",
    "https://res.cloudinary.com/lajosneto/image/upload/v1581826824/learning-ml/roc-auc-curve-details.png\" width=\"500\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
